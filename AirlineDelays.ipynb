{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Flight Delays at Chicago O'Hare International Airport\n"
      ],
      "metadata": {
        "id": "1Z5wuwEO-3Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab, we will be using 4 different machine learning models to try and classify how long a flight is going to be delayed at the Chicago O’Hare International Airport.\n",
        "Before we get started, if you want this colab to run fast, then set `DEMO` to `True`. If you want to run every line of this colab, set `DEMO` to `False`."
      ],
      "metadata": {
        "id": "0ZdoQSInitmg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEMO = True"
      ],
      "metadata": {
        "id": "x1RP0reOkvK5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Installing Dependencies\n",
        "First let’s install all the libraries and dependencies that we need to run our models."
      ],
      "metadata": {
        "id": "rvrLxLnW9joU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vv21oknD3rjl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2038d13e-da28-44d1-fe07-efe8c7819b33"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: pydotplus in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pydotplus) (3.2.3)\n",
            "Requirement already satisfied: tensorflow_decision_forests in /usr/local/lib/python3.11/dist-packages (1.11.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (2.2.2)\n",
            "Requirement already satisfied: tensorflow==2.18.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (2.18.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (1.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (1.4.0)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (3.1.1)\n",
            "Requirement already satisfied: tf-keras~=2.17 in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (2.18.0)\n",
            "Requirement already satisfied: ydf in /usr/local/lib/python3.11/dist-packages (from tensorflow_decision_forests) (0.12.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (4.14.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (1.73.1)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow==2.18.0->tensorflow_decision_forests) (0.37.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow_decision_forests) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow_decision_forests) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow_decision_forests) (2025.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (0.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow==2.18.0->tensorflow_decision_forests) (2025.6.15)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow==2.18.0->tensorflow_decision_forests) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.18.0->tensorflow_decision_forests) (0.1.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting keras-tuner==1.0.3\n",
            "  Downloading keras_tuner-1.0.3-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.0.3) (24.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.0.3) (2.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.0.3) (2.32.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.0.3) (1.15.3)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.0.3) (2.18.0)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.11/dist-packages (from keras-tuner==1.0.3) (7.34.0)\n",
            "Collecting kt-legacy (from keras-tuner==1.0.3)\n",
            "  Downloading kt_legacy-1.0.5-py3-none-any.whl.metadata (221 bytes)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython->keras-tuner==1.0.3)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (2.19.2)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython->keras-tuner==1.0.3) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.0.3) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.0.3) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.0.3) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->keras-tuner==1.0.3) (2025.6.15)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.73.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (3.8.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (5.29.5)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->keras-tuner==1.0.3) (3.1.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython->keras-tuner==1.0.3) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython->keras-tuner==1.0.3) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->keras-tuner==1.0.3) (0.2.13)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard->keras-tuner==1.0.3) (3.0.2)\n",
            "Downloading keras_tuner-1.0.3-py3-none-any.whl (96 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kt_legacy-1.0.5-py3-none-any.whl (9.6 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kt-legacy, jedi, keras-tuner\n",
            "Successfully installed jedi-0.19.2 keras-tuner-1.0.3 kt-legacy-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow\n",
        "!pip install pydotplus\n",
        "!pip install tensorflow_decision_forests\n",
        "!pip install matplotlib\n",
        "!pip install keras-tuner==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zMLRj85HS-Jk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "d08aa182-cc3d-4dc7-f281-851ad88f43fc"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<p style=\"margin:0px;\">🌲 Try <a href=\"https://ydf.readthedocs.io/en/latest/\" target=\"_blank\">YDF</a>, the successor of\n",
              "    <a href=\"https://www.tensorflow.org/decision_forests\" target=\"_blank\">TensorFlow\n",
              "        Decision Forests</a> using the same algorithms but with more features and faster\n",
              "    training!\n",
              "</p>\n",
              "<div style=\"display: flex; flex-wrap: wrap; margin:5px;max-width: 880px;\">\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            Old code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import tensorflow_decision_forests as tfdf\n",
              "\n",
              "tf_ds = tfdf.keras.pd_dataframe_to_tf_dataset(ds, label=\"l\")\n",
              "model = tfdf.keras.RandomForestModel(label=\"l\")\n",
              "model.fit(tf_ds)\n",
              "</pre>\n",
              "    </div>\n",
              "    <div style=\"width: 5px;\"></div>\n",
              "    <div style=\"flex: 1; border-radius: 10px; background-color: F0F0F0; padding: 5px;\">\n",
              "        <p\n",
              "            style=\"font-weight: bold; margin:0px;text-align: center;border-bottom: 1px solid #C0C0C0;margin-bottom: 4px;\">\n",
              "            New code</p>\n",
              "        <pre style=\"overflow-wrap: anywhere; overflow: auto; margin:0px;font-size: 9pt;\">\n",
              "import ydf\n",
              "\n",
              "model = ydf.RandomForestLearner(label=\"l\").train(ds)\n",
              "</pre>\n",
              "    </div>\n",
              "</div>\n",
              "<p style=\"margin:0px;font-size: 9pt;\">(Learn more in the <a\n",
              "        href=\"https://ydf.readthedocs.io/en/latest/tutorial/migrating_to_ydf/\" target=\"_blank\">migration\n",
              "        guide</a>)</p>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# install the dependencies\n",
        "# Python ≥3.5 is required\n",
        "\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn ≥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "np.random.seed(42)\n",
        "\n",
        "import time\n",
        "import tensorflow_decision_forests as tfdf\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "# Ignore useless warnings (see SciPy issue #5998)\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")\n",
        "import logging\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn import tree\n",
        "from sklearn.metrics import f1_score\n",
        "from six import StringIO\n",
        "import pydotplus\n",
        "from IPython.display import Image\n",
        "dot_data = StringIO()\n",
        "from sklearn import tree\n",
        "import xgboost as xgb\n",
        "from xgboost import XGBClassifier, plot_tree\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from keras.callbacks import EarlyStopping\n",
        "from matplotlib import pyplot\n",
        "\n",
        "import datetime, warnings, scipy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib.patches import ConnectionPatch\n",
        "from collections import OrderedDict\n",
        "from matplotlib.gridspec import GridSpec\n",
        "from sklearn import metrics, linear_model\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\n",
        "from scipy.optimize import curve_fit\n",
        "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "plt.style.use('fivethirtyeight')\n",
        "mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"last_expr\"\n",
        "pd.options.display.max_columns = 50\n",
        "%matplotlib inline\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  # FATAL\n",
        "logging.getLogger('tensorflow').setLevel(logging.FATAL)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0phDEvuWCLF8"
      },
      "source": [
        "###Class made for Plotting Graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "K3bSriQHCJor"
      },
      "outputs": [],
      "source": [
        "class Figure_style():\n",
        "    #_________________________________________________________________\n",
        "    def __init__(self, size_x = 11, size_y = 5, nrows = 1, ncols = 1):\n",
        "        sns.set_style(\"white\")\n",
        "        sns.set_context(\"notebook\", font_scale=1.2, rc={\"lines.linewidth\": 2.5})\n",
        "        self.fig, axs = plt.subplots(nrows = nrows, ncols = ncols, figsize=(size_x,size_y,))\n",
        "        #________________________________\n",
        "        # convert self.axs to 2D array\n",
        "        if nrows == 1 and ncols == 1:\n",
        "            self.axs = np.reshape(axs, (1, -1))\n",
        "        elif nrows == 1:\n",
        "            self.axs = np.reshape(axs, (1, -1))\n",
        "        elif ncols == 1:\n",
        "            self.axs = np.reshape(axs, (-1, 1))\n",
        "    #_____________________________\n",
        "    def pos_update(self, ix, iy):\n",
        "        self.ix, self.iy = ix, iy\n",
        "    #_______________\n",
        "    def style(self):\n",
        "        self.axs[self.ix, self.iy].spines['right'].set_visible(False)\n",
        "        self.axs[self.ix, self.iy].spines['top'].set_visible(False)\n",
        "        self.axs[self.ix, self.iy].yaxis.grid(color='lightgray', linestyle=':')\n",
        "        self.axs[self.ix, self.iy].xaxis.grid(color='lightgray', linestyle=':')\n",
        "        self.axs[self.ix, self.iy].tick_params(axis='both', which='major',\n",
        "                                               labelsize=10, size = 5)\n",
        "    #________________________________________\n",
        "    def draw_legend(self, location='upper right'):\n",
        "        legend = self.axs[self.ix, self.iy].legend(loc = location, shadow=True,\n",
        "                                        facecolor = 'g', frameon = True)\n",
        "        legend.get_frame().set_facecolor('whitesmoke')\n",
        "    #_________________________________________________________________________________\n",
        "    def cust_plot(self, x, y, color='b', linestyle='-', linewidth=1, marker=None, label=''):\n",
        "        if marker:\n",
        "            markerfacecolor, marker, markersize = marker[:]\n",
        "            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n",
        "                                linewidth = linewidth, marker = marker, label = label,\n",
        "                                markerfacecolor = markerfacecolor, markersize = markersize)\n",
        "        else:\n",
        "            self.axs[self.ix, self.iy].plot(x, y, color = color, linestyle = linestyle,\n",
        "                                        linewidth = linewidth, label=label)\n",
        "        self.fig.autofmt_xdate()\n",
        "    #________________________________________________________________________\n",
        "    def cust_plot_date(self, x, y, color='lightblue', linestyle='-',\n",
        "                       linewidth=1, markeredge=False, label=''):\n",
        "        markeredgewidth = 1 if markeredge else 0\n",
        "        self.axs[self.ix, self.iy].plot_date(x, y, color='lightblue', markeredgecolor='grey',\n",
        "                                  markeredgewidth = markeredgewidth, label=label)\n",
        "    #________________________________________________________________________\n",
        "    def cust_scatter(self, x, y, color = 'lightblue', markeredge = False, label=''):\n",
        "        markeredgewidth = 1 if markeredge else 0\n",
        "        self.axs[self.ix, self.iy].scatter(x, y, color=color,  edgecolor='grey',\n",
        "                                  linewidths = markeredgewidth, label=label)\n",
        "    #___________________________________________\n",
        "    def set_xlabel(self, label, fontsize = 14):\n",
        "        self.axs[self.ix, self.iy].set_xlabel(label, fontsize = fontsize)\n",
        "    #___________________________________________\n",
        "    def set_ylabel(self, label, fontsize = 14):\n",
        "        self.axs[self.ix, self.iy].set_ylabel(label, fontsize = fontsize)\n",
        "    #____________________________________\n",
        "    def set_xlim(self, lim_inf, lim_sup):\n",
        "        self.axs[self.ix, self.iy].set_xlim([lim_inf, lim_sup])\n",
        "    #____________________________________\n",
        "    def set_ylim(self, lim_inf, lim_sup):\n",
        "        self.axs[self.ix, self.iy].set_ylim([lim_inf, lim_sup])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading the dataset\n",
        "We cleaned up the dataset locally and loaded it on github. Here we are loading the dataset onto this colab from github for additional datapreprocessing. Multiple copies of the dataframes are created since the NN and the trees need the data in different formats."
      ],
      "metadata": {
        "id": "8gxqO_f_DzOr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load the dataset\n",
        "dataset_url = \"https://raw.githubusercontent.com/lee-anh/FlightPredictions/main/flightsResult1.csv\"\n",
        "dataset_url1 =  \"https://raw.githubusercontent.com/lee-anh/FlightPredictions/main/flightsResult2.csv\"\n",
        "\n",
        "# For NNs\n",
        "df = pd.read_csv(dataset_url)\n",
        "df = df.append(pd.read_csv(dataset_url1))\n",
        "\n",
        "# For gradient boosted model\n",
        "df1 = df.copy()\n",
        "\n",
        "# look at the headers\n",
        "data_top = df1.head()\n",
        "data_top\n",
        "\n"
      ],
      "metadata": {
        "id": "jPFIGrVq9_ER",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "61914b63-77e8-4bb8-a0e9-4051cb8947e1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'append'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-4243376466.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# For NNs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_url1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# For gradient boosted model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   6297\u001b[0m         ):\n\u001b[1;32m   6298\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6299\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6301\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfinal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'append'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Exploration\n",
        "Before we get started, let's explore the data to see what features might be especially important for predicing flights! To start, we will drop all data points that have NaN values. The we will plot correlations between delay time and weather variables such as min temperature, max temperature, temperature difference, and precipitation."
      ],
      "metadata": {
        "id": "PKEV4wjgEjaV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixx21jUQRpLN"
      },
      "outputs": [],
      "source": [
        "# Check how complete the data is (aka % not NaN)\n",
        "missing_df = df.isnull().sum(axis=0).reset_index()\n",
        "missing_df.columns = ['variable', 'missing values']\n",
        "missing_df['filling factor (%)']=(df.shape[0]-missing_df['missing values'])/df.shape[0]*100\n",
        "missing_df.sort_values('filling factor (%)').reset_index(drop = True)\n",
        "\n",
        "\n",
        "# Clean data to remove Nan values\n",
        "df = df.dropna()\n",
        "df = df.reset_index(drop=True)\n",
        "\n",
        "#Take a look at average weather delay times and their distribution in this dataset\n",
        "df[\"WEATHER_DELAY\"].plot(kind=\"hist\", range=[0, 70], ylim = (0,1000))\n",
        "print(\"MAXIMUM DELAY: \", df[\"WEATHER_DELAY\"].max())\n",
        "print(\"MINIMUM DELAY: \", df[\"WEATHER_DELAY\"].min())\n",
        "print(\"AVERAGE DELAY: \", df[\"WEATHER_DELAY\"].mean(skipna = True))\n",
        "\n",
        "# Lets take a look to see how time of day effects the delays in the dataset\n",
        "\n",
        "# convert scheduled departure to minutes past 12 am\n",
        "df['SCHEDULED_DEPARTURE'] = df['SCHEDULED_DEPARTURE'].map(lambda a : (a % 100) + ((a/100)*60) )\n",
        "\n",
        "fig1 = Figure_style(11, 5, 1, 1)\n",
        "fig1.pos_update(0, 0)\n",
        "fig1.cust_scatter(df['SCHEDULED_DEPARTURE'], df['WEATHER_DELAY'], label='o')\n",
        "fig1.style()\n",
        "fig1.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig1.set_xlabel('Departure Time (minutes past 12 am)', fontsize = 14)\n",
        "fig1.set_xlim(0, 1440)\n",
        "fig1.set_ylim(-15, 260)\n",
        "\n",
        "# This looks interesting, the peak delays seem to have a trace of some patern\n",
        "# lets look closer 1000 and 1200\n",
        "fig2 = Figure_style(11, 5, 1, 1)\n",
        "fig2.pos_update(0, 0)\n",
        "fig2.cust_scatter(df['SCHEDULED_DEPARTURE'], df['WEATHER_DELAY'], label='o')\n",
        "fig2.style()\n",
        "fig2.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig2.set_xlabel('Departure Time (minutes past 12 am)', fontsize = 14)\n",
        "fig2.set_xlim(1000, 1200)\n",
        "fig2.set_ylim(-15, 400)\n",
        "\n",
        "# Ok looks like it could be represented by a function\n",
        "\n",
        "# Lets look at Delays due to Snow and Precipiatation\n",
        "\n",
        "fig3 = Figure_style(11, 5, 1, 1)\n",
        "fig3.pos_update(0, 0)\n",
        "fig3.cust_scatter(df['SNOW'], df['WEATHER_DELAY'], label='o')\n",
        "fig3.style()\n",
        "fig3.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig3.set_xlabel('Snow (inches) ', fontsize = 14)\n",
        "fig3.set_xlim(0, 20)\n",
        "fig3.set_ylim(-15, 400)\n",
        "\n",
        "fig4 = Figure_style(11, 5, 1, 1)\n",
        "fig4.pos_update(0, 0)\n",
        "fig4.cust_scatter(df['SNOW'], df['WEATHER_DELAY'], label='o')\n",
        "fig4.style()\n",
        "fig4.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig4.set_xlabel('Precipiatation (inches) ', fontsize = 14)\n",
        "fig4.set_xlim(0, 20)\n",
        "fig4.set_ylim(-15, 400)\n",
        "\n",
        "# Let's look at temp\n",
        "fig5 = Figure_style(11, 5, 1, 1)\n",
        "fig5.pos_update(0, 0)\n",
        "fig5.cust_scatter(df['TMIN'], df['WEATHER_DELAY'], label='o')\n",
        "fig5.style()\n",
        "fig5.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig5.set_xlabel('Temp-Min (°F)', fontsize = 14)\n",
        "fig5.set_xlim(-20, 120)\n",
        "fig5.set_ylim(-15, 400)\n",
        "\n",
        "# INTERESTING SEEMS LIKE A PARABOLA LIKE SHAPE\n",
        "fig6 = Figure_style(11, 5, 1, 1)\n",
        "fig6.pos_update(0, 0)\n",
        "fig6.cust_scatter(df['TMAX'], df['WEATHER_DELAY'], label='o')\n",
        "fig6.style()\n",
        "fig6.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig6.set_xlabel('Temp-Max (°F)', fontsize = 14)\n",
        "fig6.set_xlim(-20, 120)\n",
        "fig6.set_ylim(-15, 400)\n",
        "\n",
        "# SAME HERE\n",
        "fig7 = Figure_style(11, 5, 1, 1)\n",
        "fig7.pos_update(0, 0)\n",
        "fig7.cust_scatter(df['TMAX'] - df['TMIN'] , df['WEATHER_DELAY'], label='o')\n",
        "fig7.style()\n",
        "fig7.set_ylabel('Delay (minutes)', fontsize = 14)\n",
        "fig7.set_xlabel('Temp-Dif (°F)', fontsize = 14)\n",
        "fig7.set_xlim(-20, 120)\n",
        "fig7.set_ylim(-15, 400)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n",
        "We must process the data differently for NN and for our tree based models.\n",
        "\n",
        "For the NNs, we will one-hot the categorical data such as the location of the flight and the name of the airport it is flying to. Our `y` (the WEATHER_DELAYs) need to be separated out from the input `X` and then classified into 9 classes, which are time brackets. Then we split the data into our training, validation, and test sets.\n",
        "\n",
        "For the trees, we specify the target variable using `label` and there is no need so separate it out. We classify `y` into 9 separate classes, just as we did for the NNs. Then We create 2 different traininging, validation, and test sets. One set will be used by the Gradient Boosted Tree model and the other will be used by the Random Forest Model. A different ratio is needed for the Random Forest model because it takes a long time to run."
      ],
      "metadata": {
        "id": "vtHtg7OF_jkK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mhT6WdfLfWk"
      },
      "outputs": [],
      "source": [
        "# DATA PREPROCESSING\n",
        "\n",
        "##################### For NN ####################################\n",
        "def OneHot(df, name):\n",
        "  df[\"DESTINATION_AIRPORT\"] = df[\"DESTINATION_AIRPORT\"].astype('category')\n",
        "  df['Gen_new'] = df[\"DESTINATION_AIRPORT\"].cat.codes\n",
        "  df[\"AIRLINE\"] = df[\"AIRLINE\"].astype('category')\n",
        "  df['Air_new'] = df[\"AIRLINE\"].cat.codes\n",
        "  # Create an instance of One-hot-encoder\n",
        "  enc = OneHotEncoder()\n",
        "\n",
        "# Passing encoded columns\n",
        "  enc_data = pd.DataFrame(enc.fit_transform(\n",
        "    df[['Gen_new','Air_new']]).toarray())\n",
        "\n",
        "  # Merge with main\n",
        "  New_df = df.join(enc_data)\n",
        "  New_df = New_df.drop(\"Gen_new\", axis='columns')\n",
        "  New_df = New_df.drop(\"DESTINATION_AIRPORT\", axis='columns')\n",
        "  New_df = New_df.drop(\"Air_new\", axis='columns')\n",
        "  New_df = New_df.drop(\"AIRLINE\", axis='columns')\n",
        "  return New_df\n",
        "\n",
        "\n",
        "# drop the first column, which is just an enumeration\n",
        "df.drop(columns=df.columns[0], axis=1,  inplace=True)\n",
        "\n",
        "# this helper function classsifies the delays into 9 different classes\n",
        "def classify_delays(dt):\n",
        "    if(dt < 1):\n",
        "        return 0\n",
        "    elif(dt < 4):\n",
        "        return 1\n",
        "    elif(dt < 11):\n",
        "        return 2\n",
        "    elif(dt < 17):\n",
        "        return 3\n",
        "    elif(dt < 25):\n",
        "        return 4\n",
        "    elif (dt < 37):\n",
        "        return 5\n",
        "    elif (dt < 59):\n",
        "        return 6\n",
        "    elif (dt < 130):\n",
        "        return 7\n",
        "    else:\n",
        "        return 8\n",
        "\n",
        "\n",
        "y = df[\"WEATHER_DELAY\"].map(classify_delays )\n",
        "\n",
        "\n",
        "df = df.drop([\"TAIL_NUMBER\"], axis=\"columns\")\n",
        "df = df.drop([\"ORIGIN_AIRPORT\"], axis=\"columns\")\n",
        "\n",
        "# Onehotting every categorical var\n",
        "df = OneHot(df, \"DESTINATION_AIRPORT\")\n",
        "\n",
        "# X\n",
        "df.columns = df.columns.astype(str)\n",
        "X = df.drop([\"WEATHER_DELAY\"], axis=\"columns\")\n",
        "\n",
        "# split the dataset to train, validation, and test - numpy format\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size = 0.25, random_state = 1)\n",
        "\n",
        "\n",
        "X_train=np.asarray(X_train).astype(int)\n",
        "X_val=np.asarray(X_val).astype(int)\n",
        "\n",
        "\n",
        "\n",
        "#####################  gbt and random forest ###########################\n",
        "label = \"WEATHER_DELAY\"\n",
        "\n",
        "# change delays to 9 classes\n",
        "df1[label] = df1[label].map(classify_delays)\n",
        "\n",
        "# split the dataset to train, validation, and test - pandas format\n",
        "def split_dataset(dataset, test_ratio=0.30):\n",
        "  \"\"\"Splits a panda dataframe in two.\"\"\"\n",
        "  test_indices = np.random.rand(len(dataset)) < test_ratio\n",
        "  return dataset[~test_indices], dataset[test_indices]\n",
        "\n",
        "# df2 will be a copy for the random forest model\n",
        "df2 = df1.copy()\n",
        "\n",
        "\n",
        "# gbt\n",
        "ds_pd_train, ds_pd_test = split_dataset(df1, 0.2)\n",
        "ds_pd_test_y = ds_pd_test[label]\n",
        "ds_pd_train, ds_pd_val= split_dataset(ds_pd_train, 0.25)\n",
        "print(\"For XGB: {} examples in training, {} examples for testing, {} examples in validation.\".format(\n",
        "    len(ds_pd_train), len(ds_pd_val), len(ds_pd_test)))\n",
        "\n",
        "# random forest\n",
        "ds_pd_train2, ds_pd_test2 = split_dataset(df2, 0.2)\n",
        "ds_pd_train2, ds_pd_val2= split_dataset(ds_pd_train2, 0.95)\n",
        "print(\"For Random Forest: {} examples in training, {} examples for testing, {} examples in validation.\".format(\n",
        "    len(ds_pd_train2), len(ds_pd_val2), len(ds_pd_test2)))\n",
        "\n",
        "\n",
        "# convert to tensorflow datasets\n",
        "ds_train = tfdf.keras.pd_dataframe_to_tf_dataset(ds_pd_train, label=label)\n",
        "ds_test = tfdf.keras.pd_dataframe_to_tf_dataset(ds_pd_test, label=label)\n",
        "ds_val =  tfdf.keras.pd_dataframe_to_tf_dataset(ds_pd_val, label=label)\n",
        "\n",
        "# convert to tensorflow datasets\n",
        "ds_train2 = tfdf.keras.pd_dataframe_to_tf_dataset(ds_pd_train2, label=label)\n",
        "ds_test2 = tfdf.keras.pd_dataframe_to_tf_dataset(ds_pd_test2, label=label)\n",
        "ds_val2 =  tfdf.keras.pd_dataframe_to_tf_dataset(ds_pd_val2, label=label)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Our Models\n",
        "In this section we will be training and tuning multiple types of models:\n",
        "- NN\n",
        "- Gradient Boosted Tree\n",
        "- Random Forest\n",
        "- KNN\n"
      ],
      "metadata": {
        "id": "mJ63Vn6THdZF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Model\n"
      ],
      "metadata": {
        "id": "wmsEhp9DHuo_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CLm8fqf5x8N9"
      },
      "outputs": [],
      "source": [
        "# FUNCTION TO RUN MODEL\n",
        "\n",
        "def runModel(hidden_layers, LR, neurons):\n",
        "  # Specify the model.\n",
        "  # NEURAL NETWORK MODEL\n",
        "  encoder = OneHotEncoder()\n",
        "\n",
        "  encoded_Y_train = encoder.fit(y_train.values.reshape(-1,1))\n",
        "  encoded_Y_train = encoded_Y_train.transform(y_train.values.reshape(-1,1)).toarray()\n",
        "  encoded_Y_val = encoder.fit(y_val.values.reshape(-1,1))\n",
        "  encoded_Y_val = encoded_Y_val.transform(y_val.values.reshape(-1,1)).toarray()\n",
        "\n",
        "\n",
        "  simple_model = keras.models.Sequential()\n",
        "  es = EarlyStopping(monitor='val_loss', mode='min', verbose=1)\n",
        "\n",
        "  simple_model.add(keras.layers.Dense(190, activation = 'tanh', kernel_regularizer='l1_l2'))\n",
        "  simple_model.add(Dropout(0.2))\n",
        "  for i in range(1,hidden_layers-5):\n",
        "    if (i % 2 == 0):\n",
        "      simple_model.add(keras.layers.Dense(neurons, activation = 'tanh', kernel_regularizer='l1_l2'))\n",
        "      simple_model.add(Dropout(0.2))\n",
        "    else:\n",
        "      simple_model.add(keras.layers.Dense(neurons, activation = 'relu', kernel_regularizer='l1_l2'))\n",
        "      simple_model.add(Dropout(0.2))\n",
        "  simple_model.add(keras.layers.Dense(9, activation = 'softmax'))\n",
        "\n",
        "  opt = keras.optimizers.Adam(learning_rate= LR)\n",
        "\n",
        "  simple_model.compile(optimizer=opt, loss='categorical_crossentropy', metrics = ['categorical_accuracy'])\n",
        "  simple_model.summary\n",
        "  history  = simple_model.fit(X_train, encoded_Y_train, epochs=10, validation_data=(X_val, encoded_Y_val),callbacks=[es])\n",
        "\n",
        "  return  simple_model, max(history.history['val_categorical_accuracy'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Hyperparameter Tuning for NN"
      ],
      "metadata": {
        "id": "uPcrJelHXKbe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KIMw0YF0-1DI"
      },
      "outputs": [],
      "source": [
        "#TUNING NN Model for optimal amout of hidden layers\n",
        "\n",
        "def repeat_List(a,x):\n",
        "  return list(np.repeat(a,x))\n",
        "\n",
        "\n",
        "if (not DEMO):\n",
        "  list_valac = []\n",
        "  hidden_layers = np.arange(1, 30, 5).tolist()\n",
        "  neurons = np.arange(30, 290, 50).tolist()\n",
        "  learning_rates = [0.01, 0.05] # do we want to include this guy too?\n",
        "  max_hid = hidden_layers[0]\n",
        "  max_neurons = neurons[0]\n",
        "  max_acc = 0.0\n",
        "  counter = 0\n",
        "\n",
        "\n",
        "  for h in hidden_layers:\n",
        "\n",
        "    for l in neurons:\n",
        "      print(\"Trial: \", counter)\n",
        "      print(\"h: {} hidden layers, l: {} neurons\".format(h, l))\n",
        "      model, score = runModel(h,0.01, l)\n",
        "      list_valac.append(score)\n",
        "      if (score > max_acc):\n",
        "       max_acc = score\n",
        "       max_hid = h\n",
        "       max_neurons = l\n",
        "      counter = counter + 1\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if (not DEMO):\n",
        "# plotting\n",
        "  fig = plt.figure()\n",
        "\n",
        "  ax = plt.axes(projection ='3d')\n",
        "\n",
        "  print(\" hidden layers\")\n",
        "  print(hidden_layers)\n",
        "  print(\"Neurons\")\n",
        "  print(neurons)\n",
        "  print(\"Val accuracy\")\n",
        "  print(list_valac)\n",
        "  ax.plot3D(repeat_List(hidden_layers, len(neurons)), repeat_List(neurons, len(hidden_layers)), list_valac, 'green')\n",
        "  ax.set_title('3D line plot Hyper-Parameter Tuning')\n",
        "  ax.set_xlabel(\"# of Hidden Layers\")\n",
        "  ax.set_ylabel(\"# of Nuerons\")\n",
        "  ax.set_zlabel(\"Val Accuracy\")\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "87Q3y79gVrEn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optimal NN model"
      ],
      "metadata": {
        "id": "CcZokXKbXc2Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, score = runModel(11,0.01, 50)\n",
        "\n",
        "print(\"Val Accuracy:\")\n",
        "print(score)\n",
        "\n",
        "#formatting test vals\n",
        "\n",
        "encoder = OneHotEncoder()\n",
        "\n",
        "encoded_Y_test = encoder.fit(y_test.values.reshape(-1,1))\n",
        "encoded_Y_test = encoded_Y_test.transform(y_test.values.reshape(-1,1)).toarray()\n",
        "\n",
        "test_loss, test_accuracy = model.evaluate(X_test, encoded_Y_test)\n",
        "\n",
        "# Print the test loss and accuracy\n",
        "print('Test Loss:', test_loss)\n",
        "print('Test Accuracy:', test_accuracy)\n",
        "\n",
        "\n",
        "\n",
        "# Load the trained model\n",
        "\n",
        "# Get the test set predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "# Compute the F1 score for the test set\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"f1 score\", f1)\n"
      ],
      "metadata": {
        "id": "_PQK7FAJXchk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Boosted Trees\n",
        "We will now try to use a Graident Boosted Tree. The Gradient Boosted tree trains multiple tree where new trees correct for the errors in previous trees. This is especially beneficial for our use case because over the course of the year, flight routes are repeated by the same airlines and some of these flights are delayed and others aren't so the cause of the delay coule be more subtle and the gradient boosted trees could capture these subtleties.\n",
        "\n",
        "We also use the hyperparemeter_template of \"benchmark_rank1\", which is the template suggested by the official TensorFlow documentation."
      ],
      "metadata": {
        "id": "B-liIqzvHzyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Boosted Tree with 9 classes\n",
        "\n",
        "if (not DEMO):\n",
        "  tuner = tfdf.tuner.RandomSearch(num_trials=10)\n",
        "\n",
        "\n",
        "  model_1 = tfdf.keras.GradientBoostedTreesModel(hyperparameter_template=\"benchmark_rank1\")\n",
        "  model_1.fit(ds_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "MeAm-riCBHjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GBT analytics\n",
        "if (not DEMO):\n",
        "\n",
        "\n",
        "  print(model_1.summary())\n",
        "\n",
        "  tfdf.model_plotter.plot_model_in_colab(model_1, tree_idx=0, max_depth=3)\n",
        "  model_1.compile(metrics=[\"accuracy\"])\n",
        "  evaluation = model_1.evaluate(ds_test, return_dict=True)\n",
        "\n",
        "\n",
        "  for name, value in evaluation.items():\n",
        "    print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "  print(\"input features\")\n",
        "  model_1.make_inspector().features()\n",
        "\n",
        "  print(\"feature importance\")\n",
        "  model_1.make_inspector().variable_importances()\n",
        "  # Load the trained model\n",
        "\n",
        "  # F1 Score\n",
        "  y_pred = model_1.predict(ds_test)\n",
        "\n",
        "\n",
        " # Convert the predicted probabilities to class labels\n",
        "  y_pred = np.argmax(y_pred, axis=1)\n",
        "  y_true = [y for x, y in ds_test]\n",
        "  y_true = tf.concat(y_true, axis=0).numpy()\n",
        "  f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "  print(\"F1 score:\", f1)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "45Pa2iHzBHa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GBT plot\n",
        "if (not DEMO):\n",
        "  tfdf.model_plotter.plot_model_in_colab(model_1, tree_idx=0, max_depth=3)"
      ],
      "metadata": {
        "id": "LrPvSp16GvP8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GBT graph\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if (not DEMO):\n",
        "  logs = model_1.make_inspector().training_logs()\n",
        "\n",
        "  plt.figure(figsize=(12, 4))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "  plt.xlabel(\"Number of trees\")\n",
        "  plt.ylabel(\"Accuracy (out-of-bag)\")\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
        "  plt.xlabel(\"Number of trees\")\n",
        "  plt.ylabel(\"Logloss (out-of-bag)\")\n",
        "\n",
        "  plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "D1Gb69pDGqCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Random Forest Model\n",
        "Random forest models train multiple trees at once. These trees are forced to pick from a random subset of features which diversifies them. Together, the trees create a forest that is collectively more stable and potentially more accurate than the rest of the trees. A major disadvantage of random forest models is that they take a long time to train. We trained our random forest model on a subset (~20%) of the training data.\n"
      ],
      "metadata": {
        "id": "kUI3r0xELeEY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest\n",
        "model_2 = tfdf.keras.RandomForestModel(verbose=1)\n",
        "\n",
        "# Train the model.\n",
        "model_2.fit(ds_train2)"
      ],
      "metadata": {
        "id": "AarAeQ1uF8Lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest analytics\n",
        "\n",
        "print(\"model summary\")\n",
        "model_2.summary()\n",
        "\n",
        "print(\"input features\")\n",
        "model_2.make_inspector().features()\n",
        "\n",
        "print(\"feature importance\")\n",
        "model_2.make_inspector().variable_importances()\n",
        "\n",
        "print(\"training logs\")\n",
        "model_2.make_inspector().training_logs()\n",
        "\n",
        "y_pred = model_2.predict(ds_test2)\n",
        "\n",
        "model_2.compile(metrics=[\"accuracy\"])\n",
        "evaluation = model_2.evaluate(ds_test2, return_dict=True)\n",
        "print()\n",
        "\n",
        "for name, value in evaluation.items():\n",
        "  print(f\"{name}: {value:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        " # Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "y_true = [y for x, y in ds_test2]\n",
        "y_true = tf.concat(y_true, axis=0).numpy()\n",
        "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "print(\"F1 score:\", f1)"
      ],
      "metadata": {
        "id": "xiKu0feOGAr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest tree plot\n",
        "tfdf.model_plotter.plot_model_in_colab(model_2, tree_idx=0, max_depth=3)"
      ],
      "metadata": {
        "id": "Y9QsXt1pGNz9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random forest Graph\n",
        "\n",
        "logs = model_2.make_inspector().training_logs()\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.accuracy for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Accuracy (out-of-bag)\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot([log.num_trees for log in logs], [log.evaluation.loss for log in logs])\n",
        "plt.xlabel(\"Number of trees\")\n",
        "plt.ylabel(\"Logloss (out-of-bag)\")\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UGRh9uGXGYCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###KNN Model With Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "UALYXWUVMX-2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "azIhFbSL5KoA"
      },
      "outputs": [],
      "source": [
        "# knn model\n",
        "if (not DEMO):\n",
        "    # HYPER PARAMETER TUNING FOR THE k VALUE\n",
        "\n",
        "    k_values = [i for i in range (1,20)]\n",
        "    scores = []\n",
        "    #df.drop([\"WEATHER_DELAY\"], axis=\"columns\")\n",
        "\n",
        "    # REDUCIING SAMPLE SIZE SO TREE TAKES LESS PROCESSING POWER\n",
        "    Xf = df.sample(n=50000)\n",
        "    Xf = Xf.dropna()\n",
        "    Xf = Xf.reset_index(drop = True)\n",
        "    X = Xf.drop([\"WEATHER_DELAY\"], axis=\"columns\")\n",
        "    y = Xf[\"WEATHER_DELAY\"]\n",
        "\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "    for k in k_values:\n",
        "        knn = KNeighborsClassifier(n_neighbors=k)\n",
        "        score = cross_val_score(knn, X, y, cv=5)\n",
        "        scores.append(np.mean(score))\n",
        "\n",
        "    sns.lineplot(x = k_values, y = scores, marker = 'o')\n",
        "    plt.xlabel(\"K Values\")\n",
        "    plt.ylabel(\"Accuracy Score\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Optimal KNN Model"
      ],
      "metadata": {
        "id": "tvCGBPWx3Zxm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "score = cross_val_score(knn, X, y, cv=5)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "y_pred = np.argmax(y_pred, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"accuracy score\", accuracy)\n",
        "# Compute the F1 score for the test set\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "print(\"f1 score\", f1)"
      ],
      "metadata": {
        "id": "b2eJ8wbb02Q3"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}